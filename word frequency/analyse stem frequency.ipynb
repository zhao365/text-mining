{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll deal with text from tv series and from the given text we want to:\n",
    "- get word stem list ordered by frequency.\n",
    "- show example sentences for each words according to the above stem list.\n",
    "the results are save in a csv file and a txt file respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysrt\n",
    "from string import punctuation\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pysrt to read subtitle files in srt format.\n",
    "def get_subs(path):\n",
    "    subs = pysrt.open(path)\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word in the subtitle, save its location (index of sentence in the subtitle).\n",
    "def get_word_loc_dic(subs,num):\n",
    "    punctuation_remove = punctuation.replace(\"'\", \"\") \n",
    "    pattern = r\"[{}]\".format(punctuation_remove) \n",
    "    text = []\n",
    "    for i in range(len(subs)):\n",
    "        line = subs[i].text\n",
    "        line = line.lower()\n",
    "        line = re.sub(pattern,' ', line)\n",
    "        line = re.sub('[0-9]+', '', line)\n",
    "        words = line.split( )\n",
    "        text.append(words)\n",
    "    word_loc_dic = {}\n",
    "    for index, row in enumerate(text):\n",
    "        for word in row:\n",
    "            if word in word_loc_dic:\n",
    "                word_loc_dic[word].extend([(num,index)])\n",
    "            else:\n",
    "                word_loc_dic[word] = [(num, index)]\n",
    "    return word_loc_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are several subtitles and we save words and their locations in several dictionaries, we want to combine\n",
    "# those dictionaries\n",
    "def combine_dict(dict_list):\n",
    "    dict_num = len(dict_list)\n",
    "    combined_dict = dict_list[0]\n",
    "    for dic in dict_list[1:]:\n",
    "        for word, value in dic.items():\n",
    "            if word in combined_dict:\n",
    "                combined_dict[word] = combined_dict[word] + value\n",
    "            else:\n",
    "                combined_dict[word] = value\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SnowballStemmer to get stems and corresponding words.\n",
    "def get_stem_words_dic(dic):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stem_words_dic = {}\n",
    "    for word in dic.keys():\n",
    "        stem = stemmer.stem(word)\n",
    "        if stem in stem_words_dic:\n",
    "            stem_words_dic[stem].append(word)\n",
    "        else:\n",
    "            stem_words_dic[stem] = [word]\n",
    "    return stem_words_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to generate a dataframe with columns 'stem', 'words' and 'stem_freq'. In order to do that, we extract \n",
    "# necessary data from the two dictionaries (stem_words_dic, word_loc_dic) we get from previous steps. \n",
    "def get_stem_words_freq_list(stem_words_dic, word_loc_dic):\n",
    "    stem_words_freq_list = []\n",
    "    for stem, words in stem_words_dic.items():\n",
    "        stem_dict = {}\n",
    "        stem_freq = 0\n",
    "        for word in words:\n",
    "            word_freq = len(word_loc_dic[word])\n",
    "            stem_freq += word_freq\n",
    "        stem_dict[\"stem\"] = stem\n",
    "        stem_dict[\"words\"] = words\n",
    "        stem_dict[\"stem_freq\"] = stem_freq\n",
    "        stem_words_freq_list.append(stem_dict)\n",
    "    return stem_words_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the dataframe and sort it according to 'stem_freq'.\n",
    "def get_stem_words_freq_df(stem_words_freq_list):\n",
    "    df = pd.DataFrame(stem_words_freq_list, columns = ['stem', 'words','stem_freq'])\n",
    "    words_sum = df['stem_freq'].sum()\n",
    "    df['stem_freq_pct'] = df['stem_freq']/words_sum\n",
    "    df = df.sort_values(by = ['stem_freq', 'stem'], ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read subtitle files\n",
    "subs_1 = get_subs('Subtitles\\S1E1.srt')\n",
    "subs_2 = get_subs('Subtitles\\S1E2.srt')\n",
    "subs_3 = get_subs('Subtitles\\S1E3.srt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'everybody': [(3, 392), (3, 466)],\n",
       " u'four': [(1, 133), (2, 225), (2, 416), (3, 100)],\n",
       " u'hate': [(1, 268), (1, 269), (3, 9), (3, 10), (3, 183)],\n",
       " u'personally': [(2, 452)],\n",
       " u'sleep': [(3, 44), (3, 45), (3, 49)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word_location dictionaries and combine them into one.\n",
    "word_loc_1 = get_word_loc_dic(subs_1,1)\n",
    "word_loc_2 = get_word_loc_dic(subs_2,2)\n",
    "word_loc_3 = get_word_loc_dic(subs_3,3)\n",
    "word_loc_list = [word_loc_1,word_loc_2, word_loc_3]\n",
    "word_loc_combined = combine_dict(word_loc_list)\n",
    "# display first 5 items of word_loc_combined dict.\n",
    "dict(word_loc_combined.items()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'four': [u'four'],\n",
       " u'hate': [u'hate'],\n",
       " u'skeleton': [u'skeleton'],\n",
       " u'sleep': [u'sleep', u'sleeping'],\n",
       " u'sorri': [u'sorry']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stem_words dictionary\n",
    "stem_words_dic = get_stem_words_dic(word_loc_combined)\n",
    "# display first 5 items of stem_word dict.\n",
    "dict(stem_words_dic.items()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>words</th>\n",
       "      <th>stem_freq</th>\n",
       "      <th>stem_freq_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[i]</td>\n",
       "      <td>456</td>\n",
       "      <td>0.034916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>[you]</td>\n",
       "      <td>400</td>\n",
       "      <td>0.030628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>[to]</td>\n",
       "      <td>343</td>\n",
       "      <td>0.026263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>[the]</td>\n",
       "      <td>324</td>\n",
       "      <td>0.024809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it</td>\n",
       "      <td>[it, its, it's]</td>\n",
       "      <td>266</td>\n",
       "      <td>0.020368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stem            words  stem_freq  stem_freq_pct\n",
       "0    i              [i]        456       0.034916\n",
       "1  you            [you]        400       0.030628\n",
       "2   to             [to]        343       0.026263\n",
       "3  the            [the]        324       0.024809\n",
       "4   it  [it, its, it's]        266       0.020368"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get stem_words_freq dataframe\n",
    "stem_words_freq_list = get_stem_words_freq_list(stem_words_dic, word_loc_combined)\n",
    "stem_words_freq_df = get_stem_words_freq_df(stem_words_freq_list)\n",
    "# display first 5 rows of stem_words_freq_df\n",
    "stem_words_freq_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we want to summarize frequency of stems and we observe that there are contraction forms in the 'stem' column\n",
    "# such as i'm and don't, we are going to split such forms and merge them to original stems.\n",
    "def combine_lines_df(df, stem1, stem2):\n",
    "    df_update = df.copy()\n",
    "    row1_index = df[df['stem'] == stem1].index.values.astype(int)[0]    \n",
    "    row2_index = df[df['stem'] == stem2].index.values.astype(int)[0]\n",
    "    df_update.loc[row1_index, 'stem_freq'] += df_update.loc[row2_index, 'stem_freq']\n",
    "    df_update.loc[row1_index, 'stem_freq_pct'] += df_update.loc[row2_index, 'stem_freq_pct']\n",
    "    new_words_list = df_update.loc[row1_index,'words'] + df_update.loc[row2_index,'words']\n",
    "    df_update.at[row1_index,'words'] = new_words_list\n",
    "    return df_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with contractions and then drop them and reorder the dataframe\n",
    "def combine_contraction_df(df_before, order = True):\n",
    "    df = df_before.copy()\n",
    "    df = combine_lines_df(df, 'i','i\\'m')\n",
    "    df = combine_lines_df(df, 'am','i\\'m')\n",
    "    df = combine_lines_df(df, 'do','don\\'t')\n",
    "    df = combine_lines_df(df, 'not','don\\'t')\n",
    "    df = combine_lines_df(df, 'you','you\\'r')\n",
    "    df = combine_lines_df(df, 'are','you\\'r')\n",
    "    df = combine_lines_df(df, 'can','can\\'t')\n",
    "    df = combine_lines_df(df, 'not','can\\'t')\n",
    "    df = combine_lines_df(df, 'i','i\\'ll')\n",
    "    df = combine_lines_df(df, 'will','i\\'ll')\n",
    "    df = combine_lines_df(df, 'did','didn\\'t')\n",
    "    df = combine_lines_df(df, 'not','didn\\'t')\n",
    "    df = combine_lines_df(df, 'we','we\\'r')\n",
    "    df = combine_lines_df(df, 'are','we\\'r')\n",
    "    df = combine_lines_df(df, 'i','i\\'v')\n",
    "    df = combine_lines_df(df, 'have','i\\'v')\n",
    "    df = combine_lines_df(df, 'do','doesn\\'t')\n",
    "    df = combine_lines_df(df, 'not','doesn\\'t')\n",
    "    df = combine_lines_df(df, 'you','you\\'v')\n",
    "    df = combine_lines_df(df, 'have','you\\'v')\n",
    "    df = combine_lines_df(df, 'i','i\\'d')\n",
    "    df = combine_lines_df(df, 'had','i\\'d')\n",
    "    df = combine_lines_df(df, 'will','won\\'t')\n",
    "    df = combine_lines_df(df, 'not','won\\'t')\n",
    "    df = combine_lines_df(df, 'they','they\\'r')\n",
    "    df = combine_lines_df(df, 'are','they\\'r')\n",
    "    df = combine_lines_df(df, 'is','isn\\'t')\n",
    "    df = combine_lines_df(df, 'not','isn\\'t')\n",
    "    df = combine_lines_df(df, 'could','couldn\\'t')\n",
    "    df = combine_lines_df(df, 'not','couldn\\'t')\n",
    "    df = combine_lines_df(df, 'we','we\\'ll')\n",
    "    df = combine_lines_df(df, 'will','we\\'ll')\n",
    "    df = combine_lines_df(df, 'was','wasn\\'t')\n",
    "    df = combine_lines_df(df, 'not','wasn\\'t')\n",
    "    df = combine_lines_df(df, 'you','you\\'ll')\n",
    "    df = combine_lines_df(df, 'will','you\\'ll')\n",
    "    df = combine_lines_df(df, 'would','wouldn\\'t')\n",
    "    df = combine_lines_df(df, 'not','wouldn\\'t')\n",
    "    df = combine_lines_df(df, 'you','you\\'d')\n",
    "    df = combine_lines_df(df, 'had','you\\'d')\n",
    "    df = combine_lines_df(df, 'have','haven\\'t')\n",
    "    df = combine_lines_df(df, 'not','haven\\'t')\n",
    "    df = combine_lines_df(df, 'should','shouldn\\'t')\n",
    "    df = combine_lines_df(df, 'not','shouldn\\'t')\n",
    "    df = combine_lines_df(df, 'are','aren\\'t')\n",
    "    df = combine_lines_df(df, 'not','aren\\'t')\n",
    "    df = combine_lines_df(df, 'we','we\\'v')\n",
    "    df = combine_lines_df(df, 'have','we\\'v')\n",
    "    df = combine_lines_df(df, 'he','he\\'ll')\n",
    "    df = combine_lines_df(df, 'will','he\\'ll')\n",
    "    df = combine_lines_df(df, 'she','she\\'d')\n",
    "    df = combine_lines_df(df, 'had','she\\'d')\n",
    "    df = combine_lines_df(df, 'we','we\\'d')\n",
    "    df = combine_lines_df(df, 'had','we\\'d')\n",
    "    df = combine_lines_df(df, 'they','they\\'ll')\n",
    "    df = combine_lines_df(df, 'will','they\\'ll')\n",
    "    df = combine_lines_df(df, 'it','it\\'ll')\n",
    "    df = combine_lines_df(df, 'will','it\\'ll')\n",
    "    df = combine_lines_df(df, 'she','she\\'ll')\n",
    "    df = combine_lines_df(df, 'will','she\\'ll')\n",
    "    df = combine_lines_df(df, 'he','he\\'d')\n",
    "    df = combine_lines_df(df, 'had','he\\'d')\n",
    "    df = combine_lines_df(df, 'they','they\\'v')\n",
    "    df = combine_lines_df(df, 'have','they\\'v')\n",
    "    df = combine_lines_df(df, 'were','weren\\'t')\n",
    "    df = combine_lines_df(df, 'not','weren\\'t')\n",
    "    df = combine_lines_df(df, 'would','would\\'v')\n",
    "    df = combine_lines_df(df, 'have','would\\'v')\n",
    "    df = combine_lines_df(df, 'has','hasn\\'t')\n",
    "    df = combine_lines_df(df, 'not','hasn\\'t')\n",
    "    df = combine_lines_df(df, 'could','could\\'v')\n",
    "    df = combine_lines_df(df, 'have','could\\'v')\n",
    "    df = combine_lines_df(df, 'had','hadn\\'t')\n",
    "    df = combine_lines_df(df, 'not','hadn\\'t')\n",
    "    df = combine_lines_df(df, 'should','should\\'v')\n",
    "    df = combine_lines_df(df, 'have','should\\'v')\n",
    "    df = combine_lines_df(df, 'must','must\\'v')\n",
    "    df = combine_lines_df(df, 'have','must\\'v')\n",
    "    df = combine_lines_df(df, 'i','i\\'ii')\n",
    "    df = combine_lines_df(df, 'will','i\\'ii')\n",
    "    df = combine_lines_df(df, 'they','they\\'d')\n",
    "    df = combine_lines_df(df, 'had','they\\'d')\n",
    "    df = combine_lines_df(df, 'who','who\\'d')\n",
    "    df = combine_lines_df(df, 'had','who\\'d')\n",
    "    df = combine_lines_df(df, 'this','this\\'ll')\n",
    "    df = combine_lines_df(df, 'will','this\\'ll')\n",
    "    df = combine_lines_df(df, 'that','that\\'ll')\n",
    "    df = combine_lines_df(df, 'will','that\\'ll')    \n",
    "    drop_stems = stem_words_freq_df[stem_words_freq_df['stem'].str.contains(\"\\'\")]['stem'].tolist()[:38]\n",
    "    drop_stems.remove(\"ma'am\")\n",
    "    for stem in drop_stems:\n",
    "        df = df[df['stem'] != stem]\n",
    "    if order:\n",
    "        df = df.sort_values(by = ['stem_freq', 'stem'], ascending = False).reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>words</th>\n",
       "      <th>stem_freq</th>\n",
       "      <th>stem_freq_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>[i, i'm, i'll, i've, i'd, i'ii]</td>\n",
       "      <td>634</td>\n",
       "      <td>0.048545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>[you, you're, you've, you'll, you'd]</td>\n",
       "      <td>466</td>\n",
       "      <td>0.035681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>[to]</td>\n",
       "      <td>343</td>\n",
       "      <td>0.026263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>[the]</td>\n",
       "      <td>324</td>\n",
       "      <td>0.024809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not</td>\n",
       "      <td>[not, don't, can't, didn't, doesn't, won't, is...</td>\n",
       "      <td>271</td>\n",
       "      <td>0.020750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stem                                              words  stem_freq  \\\n",
       "0    i                    [i, i'm, i'll, i've, i'd, i'ii]        634   \n",
       "1  you               [you, you're, you've, you'll, you'd]        466   \n",
       "2   to                                               [to]        343   \n",
       "3  the                                              [the]        324   \n",
       "4  not  [not, don't, can't, didn't, doesn't, won't, is...        271   \n",
       "\n",
       "   stem_freq_pct  \n",
       "0       0.048545  \n",
       "1       0.035681  \n",
       "2       0.026263  \n",
       "3       0.024809  \n",
       "4       0.020750  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get clean dataframe after dealing with contractions\n",
    "stem_words_freq_df_clean = combine_contraction_df(stem_words_freq_df)\n",
    "# display first 5 rows of cleaned stem_words_freq dataframe\n",
    "stem_words_freq_df_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe with columns 'stem','words' and 'stem_freq' to csv file. For better display in csv file, \n",
    "# we remove unicode character \"u\" and brackets \"[]\".\n",
    "def remove_unicode_brackes(item):\n",
    "    item_remove_unicode = [str(i) for i in item]\n",
    "    item_remove_unicode_brackets = str(item_remove_unicode).strip(\"[]\")\n",
    "    return item_remove_unicode_brackets\n",
    "def save_csv(df, file_name):\n",
    "    df_to_csv = df.copy()\n",
    "    df_to_csv['words'] = df_to_csv['words'].apply(remove_unicode_brackes)\n",
    "    df_to_csv.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each stem we want to show example sentences for each word related to the stem.\n",
    "subs_all = [subs_1, subs_2, subs_3]\n",
    "def word_write_sentences(word, num, location = True):\n",
    "    locations = word_loc_combined[word]\n",
    "    for i in range(min(len(locations), num)):\n",
    "        epi, loc = locations[i]\n",
    "        subs = subs_all[epi-1]\n",
    "        line = subs[loc].text\n",
    "        start = subs[loc].start.to_time()\n",
    "        end = subs[loc].end.to_time()\n",
    "        file.write('Episode ' + str(epi) + \"\\n\")\n",
    "        if location:\n",
    "            file.write(str(start)+ \"\\n\")\n",
    "            file.write(str(end)+ \"\\n\")\n",
    "        file.write(line)\n",
    "        file.write(\"\\n\")\n",
    "def stem_write_words_sentences(df,stem, num, location = True):\n",
    "    row  = df[df['stem']== stem]\n",
    "    words = row['words'].item()\n",
    "    file.write(\"Stem: \" + stem + \"\\n\")\n",
    "    file.write(\"Words: \" + str(words)+ \"\\n\")\n",
    "    file.write(\"Example sentences: \"+ \"\\n\")\n",
    "    for word in words:\n",
    "        file.write(\"-\" + word + \":\"+ \"\\n\")\n",
    "        word_write_sentences(word, num, location)\n",
    "    file.write(\"\\n\")\n",
    "def freq_write_stems(df, sentence_num, stem_num, page_num, location = True):\n",
    "    length = df.shape[0]\n",
    "    if (page_num-1)*stem_num <= length:\n",
    "        for i in range((page_num-1)*stem_num, min(page_num*stem_num,length)):\n",
    "            file.write(str(i+1))\n",
    "            file.write(\"\\n\")\n",
    "            stem = df.loc[i]['stem']\n",
    "            stem_freq = df.loc[i]['stem_freq']\n",
    "            stem_freq_pct = df.loc[i]['stem_freq_pct']\n",
    "            file.write(str(stem_freq) + '\\n')\n",
    "            file.write(str(stem_freq_pct) + '\\n')\n",
    "            stem_write_words_sentences(df,stem, sentence_num, location)\n",
    "            file.write(\"----------------------------------------------------\"+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose first 20 stems with highest frequency\n",
    "stem_1_to_20 = stem_words_freq_df_clean.iloc[:20].copy()\n",
    "# export the above result into a csv file\n",
    "save_csv(stem_1_to_20, 'result_first_20stems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first 20 stems, print correponding words and several sentences\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "file = open(\"result_first_20stems_with_examples.txt\",\"w\") \n",
    "freq_write_stems(stem_1_to_20, 2, 20, 1, 1)\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
